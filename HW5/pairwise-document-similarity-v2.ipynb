{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)   \n",
    "__TA:__ Kyle Hamilton (__email__ kylehamilton _AT_ ischool.berkeley.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Document Similairity \n",
    "\n",
    "1. [Generate data](#1)\n",
    "2. [Phase 1 - Make inverted index](#2)\n",
    "3. [Phase 2 - Calculate pairwise document similarity](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "[https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf](https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf)    \n",
    "[http://stanford.edu/~rezab/papers/disco.pdf](http://stanford.edu/~rezab/papers/disco.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "### Generate some data\n",
    "[top of page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting systems_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile systems_test.txt\n",
    "docA\tbright blue butterfly forget\n",
    "docB\tbest forget bright sky\n",
    "docC\tblue sky bright sun\n",
    "docD\tunder buttefly sky hangs\n",
    "docE\tforget blue butterfly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "### Phase 1 - Make Inverted Index\n",
    "[top of page](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invertedIndexOnly.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile invertedIndexOnly.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRinvertedIndexOnly(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    \n",
    "    def steps(self):\n",
    "\n",
    "        return [MRStep(\n",
    "                    mapper=self.mapper,\n",
    "                    reducer=self.reducer)\n",
    "                ]\n",
    "    \n",
    "        \n",
    "   \n",
    "    def mapper(self,_,line):\n",
    "        '''\n",
    "        Reference:\n",
    "        \"For each term in the document,emits the term as key, and a tuple \n",
    "        consisting of the doc id and term weight as the value.\n",
    "        The MR runtime automatically handles the grouping of these tuples...\"\n",
    "        (https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf)\n",
    "        '''\n",
    "        #####################################################################\n",
    "        # docs as input, ie:\n",
    "        # doc id \\t doc value\n",
    "        # docA\tbright blue butterfly forget\n",
    "        #\n",
    "        #####################################################################\n",
    "        \n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        \n",
    "        value = value.split(\" \")\n",
    "        l = len(value)\n",
    "        for w in value:\n",
    "            # Store the length of the document to use with JACCARD (|A| + |B|)\n",
    "            yield w, (key, l)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reducer(self,key,value):\n",
    "        '''\n",
    "        Reference con't:\n",
    "        \"...which the reducer then writes out to disk, thus generating the postings.\"\n",
    "        (https://terpconnect.umd.edu/~oard/pdf/acl08elsayed2.pdf)\n",
    "        '''\n",
    "        #####################################################################\n",
    "        # Inverted Index as output, ie:\n",
    "        # \"term\" [[\"doc\",doc_length]]\n",
    "        # \"butterfly\"\t[[\"docA\", 4],[\"docD\", 4],[\"docE\", 3]]\n",
    "        #####################################################################\n",
    "        d = collections.defaultdict(list)\n",
    "        for v in value:\n",
    "            d[key].append(v)\n",
    "        yield key,d[key]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRinvertedIndexOnly.run() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "ignoring partitioner keyword arg (requires real Hadoop): 'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
      "Creating temp directory /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/invertedIndexOnly.koza.20170207.204252.766154\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/invertedIndexOnly.koza.20170207.204252.766154/output...\n",
      "Removing temp directory /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/invertedIndexOnly.koza.20170207.204252.766154...\n"
     ]
    }
   ],
   "source": [
    "!python invertedIndexOnly.py -r local systems_test.txt > systems_test_index.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Systems test - Inverted Index\n",
      "————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "         \"best\" |         docB 4 |                |               \n",
      "         \"blue\" |         docA 4 |         docC 4 |         docE 3\n",
      "       \"bright\" |         docA 4 |         docB 4 |         docC 4\n",
      "     \"buttefly\" |         docD 4 |                |               \n",
      "    \"butterfly\" |         docA 4 |         docE 3 |               \n",
      "       \"forget\" |         docA 4 |         docB 4 |         docE 3\n",
      "        \"hangs\" |         docD 4 |                |               \n",
      "          \"sky\" |         docB 4 |         docC 4 |         docD 4\n",
      "          \"sun\" |         docC 4 |                |               \n",
      "        \"under\" |         docD 4 |                |               \n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# Pretty print systems tests for generating Inverted Index\n",
    "##########################################################\n",
    "\n",
    "import json\n",
    "        \n",
    "print \"\\nSystems test - Inverted Index\"\n",
    "print \"—\"*100  \n",
    "with open(\"systems_test_index.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        word,stripe = line.split(\"\\t\")\n",
    "        stripe = json.loads(stripe)\n",
    "    \n",
    "        stripe.extend([[\"\",\"\"] for _ in xrange(3 - len(stripe))])\n",
    "        \n",
    "        print \"{0:>15} |{1:>15} |{2:>15} |{3:>15}\".format(\n",
    "            (word), stripe[0][0]+\" \"+str(stripe[0][1]), stripe[1][0]+\" \"+str(stripe[1][1]), stripe[2][0]+\" \"+str(stripe[2][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "### Phase 2 - Calculate similarity\n",
    "[top of page](#top)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard similarity\n",
    "$$\n",
    "Jaccard(A,B) = \\frac{|A \\cap B|}{|A|+|B|-|A \\cap B|}\n",
    "$$\n",
    "\n",
    "Example - using the information from the inverted index above:     \n",
    "$$\n",
    "Jaccard(docA,docB) = \\frac{2}{4+4-2} = \\frac{1}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting similarityOnly.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile similarityOnly.py\n",
    "#!~/anaconda2/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import mrjob\n",
    "from mrjob.protocol import RawProtocol\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRsimilarityOnly(MRJob):\n",
    "    \n",
    "    MRJob.SORT_VALUES = True \n",
    "    def steps(self):\n",
    "\n",
    "        JOBCONF_STEP1 = {}\n",
    "        JOBCONF_STEP2 = { \n",
    "          ######### IMPORTANT: THIS WILL HAVE NO EFFECT IN -r local MODE. MUST USE -r hadoop FOR SORTING #############\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapreduce.partition.keycomparator.options':'-k1,1nr',\n",
    "        }\n",
    "        return [MRStep(jobconf=JOBCONF_STEP1,\n",
    "                    mapper=self.mapper_pair_sim,\n",
    "                    reducer=self.reducer_pair_sim)\n",
    "                ,\n",
    "                MRStep(jobconf=JOBCONF_STEP2,\n",
    "                    mapper=None,   \n",
    "                    reducer=self.reducer_sort)\n",
    "                ]\n",
    "            \n",
    "    def mapper_pair_sim(self,_,line):\n",
    "        line = line.strip()\n",
    "        key,inv_indx = line.split(\"\\t\")\n",
    "        inv_indx = json.loads(inv_indx)\n",
    "        \n",
    "        '''\n",
    "        @input: lines (postings) from inverted index\n",
    "         \"blue\" [[\"DocA\", 4], [\"DocC\", 4], [\"DocE\", 3]]\n",
    "        \n",
    "        @output: pairs of doc and doc_length, count the number of pairs\n",
    "        make complex key and count of 1 as value:\n",
    "         DocA.3.DocB.2, 1\n",
    "         DocA.3.DocC.3, 1\n",
    "         DocB.2.DocC.3, 1\n",
    "        '''\n",
    "        \n",
    "        X = map(lambda x: x[0]+\".\"+str(x[1]) , inv_indx)\n",
    "        \n",
    "        # taking advantage of symetry, output only (a,b), but not (b,a)\n",
    "        for subset in itertools.combinations(sorted(set(X)), 2):\n",
    "            yield subset[0]+\".\"+subset[1], 1\n",
    "\n",
    "\n",
    "\n",
    "    def reducer_pair_sim(self,key,value):\n",
    "        w1,w1_len,w2,w2_len = key.split(\".\")\n",
    "        t = sum(value)\n",
    "        \n",
    "        jaccard = t / ( int(w1_len) + int(w2_len) - t )\n",
    "        \n",
    "       \n",
    "        yield jaccard, (w1+\" - \"+w2)\n",
    "    \n",
    "    \n",
    "    def reducer_sort(self,key,value):\n",
    "        for v in value:\n",
    "            yield key,v\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRsimilarityOnly.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\n",
      "Creating temp directory /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/similarityOnly.koza.20170207.092105.389095\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/local/bin/hadoop\n",
      "Using Hadoop version 2.7.2\n",
      "Copying local files to hdfs:///user/koza/tmp/mrjob/similarityOnly.koza.20170207.092105.389095/files/...\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.2:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar1243219462569908254/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob4424899413839839701.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1486380608198_0014\n",
      "  Submitted application application_1486380608198_0014\n",
      "  The url to track the job: http://localhost:8088/proxy/application_1486380608198_0014/\n",
      "  Running job: job_1486380608198_0014\n",
      "  Job job_1486380608198_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1486380608198_0014 completed successfully\n",
      "  Output directory: hdfs:///user/koza/tmp/mrjob/similarityOnly.koza.20170207.092105.389095/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=492\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=322\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=321\n",
      "\t\tFILE: Number of bytes written=369935\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=822\n",
      "\t\tHDFS: Number of bytes written=322\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5408768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2151424\n",
      "\t\tTotal time spent by all map tasks (ms)=5282\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5282\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2101\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2101\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5282\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2101\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=136\n",
      "\t\tInput split bytes=330\n",
      "\t\tMap input records=9\n",
      "\t\tMap output bytes=285\n",
      "\t\tMap output materialized bytes=327\n",
      "\t\tMap output records=15\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce input records=15\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=327\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=30\n",
      "\t\tTotal committed heap usage (bytes)=552599552\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.2:\n",
      "The have been translated as follows\n",
      " mapred.text.key.partitioner.options: mapreduce.partition.keypartitioner.options\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/hadoop-unjar5651290247871005912/] [] /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/streamjob5102663834162426132.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  Submitting tokens for job: job_1486380608198_0015\n",
      "  Submitted application application_1486380608198_0015\n",
      "  The url to track the job: http://localhost:8088/proxy/application_1486380608198_0015/\n",
      "  Running job: job_1486380608198_0015\n",
      "  Job job_1486380608198_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1486380608198_0015 completed successfully\n",
      "  Output directory: hdfs:///user/koza/tmp/mrjob/similarityOnly.koza.20170207.092105.389095/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=483\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=322\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=358\n",
      "\t\tFILE: Number of bytes written=370812\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=811\n",
      "\t\tHDFS: Number of bytes written=322\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4293632\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2142208\n",
      "\t\tTotal time spent by all map tasks (ms)=4193\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4193\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2092\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2092\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4193\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2092\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=117\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=332\n",
      "\t\tMap output materialized bytes=364\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=364\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=553648128\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/koza/tmp/mrjob/similarityOnly.koza.20170207.092105.389095/output...\n",
      "Removing HDFS temp directory hdfs:///user/koza/tmp/mrjob/similarityOnly.koza.20170207.092105.389095...\n",
      "Removing temp directory /var/folders/2f/rb8qqgd55bl77zgchyxsfl7h0000gp/T/similarityOnly.koza.20170207.092105.389095...\n"
     ]
    }
   ],
   "source": [
    "!python similarityOnly.py -r hadoop systems_test_index.txt > similarities_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Systems test - Similarity measures\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "           pair |         jaccard |         cosine |           dice |        overlap |            avg\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "    docA - docE |        0.750000 |           TODO |           TODO |           TODO |           TODO\n",
      "    docB - docC |        0.333333 |           TODO |           TODO |           TODO |           TODO\n",
      "    docA - docC |        0.333333 |           TODO |           TODO |           TODO |           TODO\n",
      "    docA - docB |        0.333333 |           TODO |           TODO |           TODO |           TODO\n",
      "    docD - docE |        0.166667 |           TODO |           TODO |           TODO |           TODO\n",
      "    docC - docE |        0.166667 |           TODO |           TODO |           TODO |           TODO\n",
      "    docB - docE |        0.166667 |           TODO |           TODO |           TODO |           TODO\n",
      "    docC - docD |        0.142857 |           TODO |           TODO |           TODO |           TODO\n",
      "    docB - docD |        0.142857 |           TODO |           TODO |           TODO |           TODO\n",
      "    docA - docD |        0.142857 |           TODO |           TODO |           TODO |           TODO\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Pretty print systems tests\n",
    "############################################\n",
    "\n",
    "import json\n",
    "print \"Systems test - Similarity measures\"\n",
    "print '—'*110\n",
    "print \"{0:>15} |{1:>16} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "        \"pair\", \"jaccard\", \"cosine\", \"dice\", \"overlap\", \"avg\")\n",
    "print '-'*110\n",
    "\n",
    "with open(\"similarities_test.txt\",\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        jaccard,stripe = line.split(\"\\t\")\n",
    "        stripe = json.loads(stripe)\n",
    "\n",
    "        print \"{0:>15} | {1:>15f} |{2:>15} |{3:>15} |{4:>15} |{5:>15}\".format(\n",
    "            stripe, float(jaccard),  \"TODO\", \"TODO\", \"TODO\", \"TODO\")\n",
    "        \n",
    "print '-'*110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
